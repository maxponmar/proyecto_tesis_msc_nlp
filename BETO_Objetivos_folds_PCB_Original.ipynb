{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfVkuWAW5r36"
      },
      "source": [
        ".64\n",
        "\n",
        "# Entrenando modelo BETO de detecta premisas y conclusiones en oraciones con PoC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0rpgtxmkET4"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfBvd7zVIbDc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ-uJS-K1wA3"
      },
      "source": [
        "**Montar disco de Gdrive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlw58ZSc1yDe",
        "outputId": "beb15a71-b9c7-44e0-b55c-75643ddc3db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# usa drive de jesusmiguelgglang1@g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_XXNkkZ1yKu",
        "outputId": "af0c0a91-694c-4c0a-dcd4-ad452e316627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold_0_test.txt   fold_2_test.txt   fold_4_test.txt   fold_6_test.txt\tfold_8_test.txt\n",
            "fold_0_train.txt  fold_2_train.txt  fold_4_train.txt  fold_6_train.txt\tfold_8_train.txt\n",
            "fold_1_test.txt   fold_3_test.txt   fold_5_test.txt   fold_7_test.txt\tfold_9_test.txt\n",
            "fold_1_train.txt  fold_3_train.txt  fold_5_train.txt  fold_7_train.txt\tfold_9_train.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Exp_Objetivos2021/folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utGQ7w2Jjdi6"
      },
      "source": [
        "Download `transformers` and install required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "klEBSPAuIaDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab548b56-9950-481f-ea35-21789d7a2163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 166111, done.\u001b[K\n",
            "remote: Counting objects: 100% (279/279), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 166111 (delta 120), reused 162 (delta 79), pack-reused 165832\u001b[K\n",
            "Receiving objects: 100% (166111/166111), 166.88 MiB | 25.09 MiB/s, done.\n",
            "Resolving deltas: 100% (125294/125294), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0.dev0)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0.dev0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.35.0.dev0)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.35.0.dev0-py3-none-any.whl size=7884083 sha256=7aff7974fc16b5f8ca3210b38720e4b7e64fd45c4825bbd794dce0bcdc2e2506\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-grhehsia/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0.dev0\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: './examples/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m/content\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iT0pMHt9K2sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287a602c-8eb2-49f4-fb8d-c4aa1de14c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval (from -r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1))\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets>=1.1.3 (from -r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2))\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.1.3->-r /content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt (line 2)) (1.16.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=ba5c8b50e6f94351f5bb087c840c90f6c3649c0d055074b613ee5311bc49c615\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: dill, multiprocess, seqeval, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15 seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "#usar info de huggigfaces\n",
        "# https://github.com/huggingface/transformers/tree/master/examples/token-classification\n",
        "\n",
        "# !wget -O 'requirements_tokenclasify.txt' \"https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/requirements.txt\"\n",
        "\n",
        "!pip install -r \"/content/drive/MyDrive/Exp_Objetivos2021/config_ner/requirements.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU8R8FeAIT2C"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8yUAiD-l64b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JELuvmOKJaq6"
      },
      "source": [
        "## 1. Download Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQXQAFDg7ZpP"
      },
      "source": [
        "Para deteccion de componentes\n",
        "\n",
        "   - dev.txt: Spanish test data for the development stage\n",
        "   - test.txt: Spanish test data\n",
        "   - train.txt: Spanish train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DPWj8VvrEeYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37d1986-7074-41df-89a4-16b11383f6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold_0_test.txt   fold_2_test.txt   fold_4_test.txt   fold_6_test.txt\tfold_8_test.txt\n",
            "fold_0_train.txt  fold_2_train.txt  fold_4_train.txt  fold_6_train.txt\tfold_8_train.txt\n",
            "fold_1_test.txt   fold_3_test.txt   fold_5_test.txt   fold_7_test.txt\tfold_9_test.txt\n",
            "fold_1_train.txt  fold_3_train.txt  fold_5_train.txt  fold_7_train.txt\tfold_9_train.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Exp_Objetivos2021/folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c4v9IKJVjL"
      },
      "source": [
        "## 2. Preprocessing - quizas usar otro modelo bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gtQ-tVQWgWe"
      },
      "source": [
        "Let's define some variables that we need for further pre-processing steps and training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z4Wl3iR0UBZv"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 120 #@param {type: \"integer\"}\n",
        "MODEL = \"dccuchile/bert-base-spanish-wwm-uncased\"  #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\",\"dccuchile/bert-base-spanish-wwm-uncased\", \"pysentimiento/robertuito-base-uncased\"]\n",
        "\n",
        "# dccuchile/bert-base-spanish-wwm-uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSfMYC7w_UpX"
      },
      "source": [
        "The script below will split sentences longer than `MAX_LENGTH` (in terms of tokens) into small ones. Otherwise, long sentences will be truncated when tokenized, causing the loss of training data and some tokens in the test set not being predicted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3gmazeojxn80"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !wget \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Exp_Objetivos2021/config_ner/preprocess.py\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RZHG9GDwFs3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648ef766-06db-4669-a8af-54822742c066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  preprocess.py  sample_data  transformers\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn10UkLQxNg3"
      },
      "outputs": [],
      "source": [
        "# !python3 preprocess.py ./drive/MyDrive/experimentosArgComp/datos_compOracionesArgC_folds/fold_0_train.txt $MODEL $MAX_LENGTH > train.txt\n",
        "# # !python3 preprocess.py ./drive/MyDrive/experimentosArgComp/datos_comp/fold_0_dev.txt $MODEL $MAX_LENGTH > dev.txt\n",
        "# !python3 preprocess.py ./drive/MyDrive/experimentosArgComp/datos_compOracionesArgC_folds/fold_0_test.txt $MODEL $MAX_LENGTH > test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJQJHNlUNMP"
      },
      "source": [
        "## 3. Labels\n",
        "\n",
        "In comp, there are have X classes of NER tags:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-DlGoiJLqAR"
      },
      "source": [
        "- O, Outside of a component\n",
        "- B-Arg, Beginning of arg\n",
        "- I-Arg, arg entity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3mlWe2BQPZ"
      },
      "source": [
        "If your dataset has different labels or more labels than CoNLL-2002/2003 datasets, run the line below to get unique labels from your data and save them into `labels.txt`. This file will be used when we start fine-tuning our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEtntk5-Wnvu"
      },
      "outputs": [],
      "source": [
        "# !cat train.txt  test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n",
        "# !cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTj6_KNwG28f"
      },
      "outputs": [],
      "source": [
        "# !head -n20 labels.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zblM76vBWXOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542aef78-a1c7-4bff-d629-97218df86ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  preprocess.py  sample_data  transformers\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G80k0O2aWwzy"
      },
      "source": [
        "# Fine-tuning Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qy9XdmlJXqE"
      },
      "source": [
        "These are the example scripts from `transformers`'s repo that we will use to fine-tune our model for NER. After 04/21/2020, Hugging Face has updated their example scripts to use a new `Trainer` class. To avoid any future conflict, let's use the version before they made these updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aPVKv-jWWMZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d12ec2a0-9526-45c9-f77f-50510360a33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1NoEUI-EGDQp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# !wget \"https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/run_ner.py\"\n",
        "# !wget \"https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/utils_ner.py\"\n",
        "# !wget \"https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/token-classification/tasks.py\"\n",
        "\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Exp_Objetivos2021/config_ner/run_ner.py\" \"/content/\"\n",
        "!cp \"/content/drive/MyDrive/Exp_Objetivos2021/config_ner/tasks.py\" \"/content/\"\n",
        "!cp \"/content/drive/MyDrive/Exp_Objetivos2021/config_ner/utils_ner.py\" \"/content/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VjEVTmraRfGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3948c17-7055-4392-bb58-d23d27351604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  preprocess.py  run_ner.py  sample_data  tasks.py  transformers  utils_ner.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VKHUmZx8YfUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d1efbe-ddaf-40d8-b91d-1ef58bbdcb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold_0_test.txt   fold_2_test.txt   fold_4_test.txt   fold_6_test.txt\tfold_8_test.txt\n",
            "fold_0_train.txt  fold_2_train.txt  fold_4_train.txt  fold_6_train.txt\tfold_8_train.txt\n",
            "fold_1_test.txt   fold_3_test.txt   fold_5_test.txt   fold_7_test.txt\tfold_9_test.txt\n",
            "fold_1_train.txt  fold_3_train.txt  fold_5_train.txt  fold_7_train.txt\tfold_9_train.txt\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/Exp_Objetivos2021/folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iCqP_-WeHQfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400d1f1a-f1dd-454e-f9fb-810291cd5181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available combinations :  1 [(3e-05, 16, 160)]\n"
          ]
        }
      ],
      "source": [
        "# grid search\n",
        " # Model training\n",
        "max_accuracy = 0\n",
        "\n",
        "# epoch 4\n",
        "epoch = [160]   # 20 , 80,  160,  200     - 2, 3, 5, 10, 30, 60, 100, 200\n",
        "# batch sizes: 8, 16, 32, 64, 128\n",
        "# learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n",
        "\n",
        "# learning_rate choices\n",
        "# learning_rates = [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05 ]\n",
        "learning_rates = [3e-5] #  3e-6\n",
        "\n",
        "# iterations choices\n",
        "# iterations = [ 100, 200, 300, 400, 500 ]\n",
        "batch = [16] # 16,32\n",
        "\n",
        "# #folds\n",
        "# folds = list(range(10))\n",
        "\n",
        "# available combination of learning_rate and iterations\n",
        "\n",
        "parameters = []\n",
        "for z  in epoch:\n",
        "    for j in batch:\n",
        "      for i in learning_rates:\n",
        "        parameters.append( ( i, j, z) )\n",
        "\n",
        "print(\"Available combinations : \",len(parameters),  parameters )\n",
        "\n",
        "\n",
        "# Applying linear searching in list of available combination\n",
        "# to achieved maximum accuracy on CV set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nU8AFzzIa22B"
      },
      "outputs": [],
      "source": [
        "  dir = \"/content/drive/MyDrive/Exp_Objetivos2021/results/\"\n",
        "\n",
        "  # /content/drive/MyDrive/experimentosArgComp/results/22abrCorpus5arg\n",
        "\n",
        "# /content/drive/MyDrive/experimentosArgComp/sample_data\n",
        "\n",
        "# /content/drive/MyDrive/experimentosArgComp/results/BETOcorpus4compPCsolo\n",
        "\n",
        "  #\n",
        "  f = open(dir+\"log_exp.txt\", \"a+\")\n",
        "  log_text = f\"results\\n\"\n",
        "  f.write(log_text)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install TensorRT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeDYwH009wOm",
        "outputId": "ef6a14fd-0413-4788-c78b-ca85f7be5027"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting TensorRT\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: TensorRT\n",
            "  Building wheel for TensorRT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TensorRT: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17281 sha256=a8a0b41970b150179e49523c8435a998d5659eac6bb48fc540901dd6e3fbaa51\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/0e/b79b08e45752491b9acfdbd69e8a609e8b2ed7640dda5a3e59\n",
            "Successfully built TensorRT\n",
            "Installing collected packages: TensorRT\n",
            "Successfully installed TensorRT-8.6.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1_WCZukBurF",
        "outputId": "bd7b1578-68cc-4d61-ae57-64558640add4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/261.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lqU8mvjIS_4D"
      },
      "outputs": [],
      "source": [
        "def read_examples_from_file(file_path):\n",
        "    \"\"\"Read words and labels from a CoNLL-2002/2003 data file.\n",
        "\n",
        "    Args:\n",
        "      file_path (str): path to NER data file.\n",
        "\n",
        "    Returns:\n",
        "      examples (dict): a dictionary with two keys: `words` (list of lists)\n",
        "        holding words in each sequence, and `labels` (list of lists) holding\n",
        "        corresponding labels.\n",
        "    \"\"\"\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        examples = {\"words\": [], \"labels\": []}\n",
        "        words = []\n",
        "        labels = []\n",
        "        for line in f:\n",
        "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
        "                if words:\n",
        "                    examples[\"words\"].append(words)\n",
        "                    examples[\"labels\"].append(labels)\n",
        "                    words = []\n",
        "                    labels = []\n",
        "            else:\n",
        "                splits = line.split(\" \")\n",
        "                words.append(splits[0])\n",
        "                if len(splits) > 1:\n",
        "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
        "                else:\n",
        "                    # Examples could have no label for mode = \"test\"\n",
        "                    labels.append(\"O\")\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbjRXJw6HQjx",
        "outputId": "8246479d-a3a2-483b-8237-9c04d2373f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'train.txt': No such file or directory\n",
            "rm: cannot remove 'test.txt': No such file or directory\n",
            "Downloading (…)okenizer_config.json: 100% 310/310 [00:00<00:00, 1.46MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 650/650 [00:00<00:00, 2.92MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 248k/248k [00:00<00:00, 5.04MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 486k/486k [00:00<00:00, 17.8MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 134/134 [00:00<00:00, 651kB/s]\n",
            "2023-10-31 02:03:15.742769: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-31 02:03:15.742816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-31 02:03:15.742858: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-31 02:03:16.843755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/31/2023 02:03:18 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "10/31/2023 02:03:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-base-ml-ner/runs/Oct31_02-03-18_3952901ecc4a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=160.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=bert-base-ml-ner,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-base-ml-ner,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:03:19,022 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:03:19,026 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-QUE\",\n",
            "    \"1\": \"I-QUE\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-QUE\": 0,\n",
            "    \"I-QUE\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:03:19,070 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:03:19,071 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:03:19,071 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:03:19,072 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:03:19,072 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:03:19,072 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:03:19,072 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer.json\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:03:19,072 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:03:19,073 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 440M/440M [00:02<00:00, 203MB/s]\n",
            "[INFO|modeling_utils.py:3083] 2023-10-31 02:03:21,683 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3861] 2023-10-31 02:03:23,728 >> Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3873] 2023-10-31 02:03:23,728 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   Creating features from dataset file at ./\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   Writing example 0 of 324\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   guid: train-1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   tokens: [CLS] el objetivo del presente trabajo es estudiar y analizar la viabilidad de la implementación de un entre ##laz ##ado en tiempo de ejecución sobre la máquina virtual del proyecto mono ademas se estudiar ##á el impacto en el tiempo de carga y ejecución de la implementación utilizando una arquitectura escalo ##nada de experimenta ##cion [SEP]\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_ids: 4 1039 3073 1081 2807 1608 1028 6661 1040 10190 1032 18137 1009 1032 13811 1009 1044 1341 4424 1082 1035 1526 1009 4242 1246 1032 6055 15417 1081 2269 6172 27424 1057 6661 30980 1039 6501 1035 1039 1526 1009 5260 1040 4242 1009 1032 13811 7553 1091 8524 20977 20143 1009 20451 1105 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 1 1 -100 -100 1 1 1 1 1 1 1 1 1 1 1 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   guid: train-2\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   tokens: [CLS] determinar la influencia de la implementación de un sistema de información basado en un enfoque de procesos en la opera ##tividad del área de créditos de la micro ##finan ##ciera crecer bajo una plataforma orientada a la w ##eb [SEP]\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_ids: 4 5679 1032 6135 1009 1032 13811 1009 1044 2074 1009 1926 6588 1035 1044 5877 1009 5676 1035 1032 2969 4767 1081 3967 1009 7701 1009 1032 6318 16796 15182 9457 2182 1091 7688 19515 1012 1032 1005 3686 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   guid: train-3\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   tokens: [CLS] desarrollar una red de área local ( lan ) que facilite la comunicación en el centro local amazonas de la universidad nacional abierta . [SEP]\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_ids: 4 5201 1091 2946 1009 3967 3592 1147 2428 1135 1041 25162 1032 3617 1035 1039 2391 3592 21281 1009 1032 2862 1954 6051 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   guid: train-4\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   tokens: [CLS] diseñar e implementar , un modelo de precisión para detectar y mitigar ph ##ishi ##ng en correos electrónicos utilizando técnicas actuales de minería de datos para aumentar el nivel de seguridad de los correos electrónicos . [SEP]\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_ids: 4 18043 1006 19118 1019 1044 4209 1009 11651 1097 13666 1040 22315 4398 29742 22396 1035 14256 12809 7553 5967 6966 1009 18596 1009 2783 1097 4928 1039 2174 1009 1955 1009 1067 14256 12809 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   guid: train-5\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   tokens: [CLS] desarrollar un sistema de monitoreo basado en la tecnología de comunicación lor ##a para determinar la calidad del agua dulce que se consume en zonas rurales [SEP]\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_ids: 4 5201 1044 2074 1009 24624 6588 1035 1032 3925 1009 3617 6756 30956 1097 5679 1032 3339 1081 2326 5525 1041 1057 23893 1035 3762 8186 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:03:23 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:03:25 - INFO - utils_ner -   Saving features into cached file ./cached_train_BertTokenizer_128\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:03:33,913 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1454: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:03:33,913 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1679] 2023-10-31 02:03:34,140 >> ***** Running training *****\n",
            "[INFO|trainer.py:1680] 2023-10-31 02:03:34,140 >>   Num examples = 324\n",
            "[INFO|trainer.py:1681] 2023-10-31 02:03:34,140 >>   Num Epochs = 160\n",
            "[INFO|trainer.py:1682] 2023-10-31 02:03:34,141 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1684] 2023-10-31 02:03:34,141 >>   Training with DataParallel so batch size has been adjusted to: 16\n",
            "[INFO|trainer.py:1685] 2023-10-31 02:03:34,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1686] 2023-10-31 02:03:34,141 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1687] 2023-10-31 02:03:34,141 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1688] 2023-10-31 02:03:34,141 >>   Number of trainable parameters = 109,262,595\n",
            "  3% 100/3360 [00:33<17:12,  3.16it/s][INFO|trainer.py:2815] 2023-10-31 02:04:07,414 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:04:07,416 >> Configuration saved in bert-base-ml-ner/checkpoint-100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:04:08,709 >> Model weights saved in bert-base-ml-ner/checkpoint-100/pytorch_model.bin\n",
            "  6% 200/3360 [01:09<17:03,  3.09it/s][INFO|trainer.py:2815] 2023-10-31 02:04:43,320 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:04:43,322 >> Configuration saved in bert-base-ml-ner/checkpoint-200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:04:44,649 >> Model weights saved in bert-base-ml-ner/checkpoint-200/pytorch_model.bin\n",
            "  9% 300/3360 [01:45<16:24,  3.11it/s][INFO|trainer.py:2815] 2023-10-31 02:05:19,382 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:05:19,384 >> Configuration saved in bert-base-ml-ner/checkpoint-300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:05:20,785 >> Model weights saved in bert-base-ml-ner/checkpoint-300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:05:24,211 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-100] due to args.save_total_limit\n",
            " 12% 400/3360 [02:22<14:03,  3.51it/s][INFO|trainer.py:2815] 2023-10-31 02:05:56,293 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:05:56,294 >> Configuration saved in bert-base-ml-ner/checkpoint-400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:05:57,419 >> Model weights saved in bert-base-ml-ner/checkpoint-400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:06:01,192 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.0231, 'learning_rate': 2.5535714285714284e-05, 'epoch': 23.81}\n",
            " 15% 500/3360 [02:59<15:57,  2.99it/s][INFO|trainer.py:2815] 2023-10-31 02:06:33,941 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:06:33,942 >> Configuration saved in bert-base-ml-ner/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:06:35,165 >> Model weights saved in bert-base-ml-ner/checkpoint-500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:06:38,563 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-300] due to args.save_total_limit\n",
            " 18% 600/3360 [03:37<15:30,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:07:11,319 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:07:11,320 >> Configuration saved in bert-base-ml-ner/checkpoint-600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:07:12,520 >> Model weights saved in bert-base-ml-ner/checkpoint-600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:07:22,371 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-400] due to args.save_total_limit\n",
            " 21% 700/3360 [04:20<14:52,  2.98it/s][INFO|trainer.py:2815] 2023-10-31 02:07:55,114 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:07:55,116 >> Configuration saved in bert-base-ml-ner/checkpoint-700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:07:56,652 >> Model weights saved in bert-base-ml-ner/checkpoint-700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:08:00,126 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-500] due to args.save_total_limit\n",
            " 24% 800/3360 [04:58<13:07,  3.25it/s][INFO|trainer.py:2815] 2023-10-31 02:08:33,082 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:08:33,083 >> Configuration saved in bert-base-ml-ner/checkpoint-800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:08:34,463 >> Model weights saved in bert-base-ml-ner/checkpoint-800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:08:43,538 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-600] due to args.save_total_limit\n",
            " 27% 900/3360 [05:42<13:56,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:09:16,741 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:09:16,742 >> Configuration saved in bert-base-ml-ner/checkpoint-900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:09:17,998 >> Model weights saved in bert-base-ml-ner/checkpoint-900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:09:28,321 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0009, 'learning_rate': 2.107142857142857e-05, 'epoch': 47.62}\n",
            " 30% 1000/3360 [06:27<13:17,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:10:01,335 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:10:01,336 >> Configuration saved in bert-base-ml-ner/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:10:02,594 >> Model weights saved in bert-base-ml-ner/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:10:13,070 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-800] due to args.save_total_limit\n",
            " 33% 1100/3360 [07:11<12:44,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:10:45,936 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:10:45,938 >> Configuration saved in bert-base-ml-ner/checkpoint-1100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:10:47,442 >> Model weights saved in bert-base-ml-ner/checkpoint-1100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:10:50,970 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-900] due to args.save_total_limit\n",
            " 36% 1200/3360 [07:49<11:34,  3.11it/s][INFO|trainer.py:2815] 2023-10-31 02:11:24,092 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:11:24,093 >> Configuration saved in bert-base-ml-ner/checkpoint-1200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:11:25,562 >> Model weights saved in bert-base-ml-ner/checkpoint-1200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:11:33,663 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1000] due to args.save_total_limit\n",
            " 39% 1300/3360 [08:32<11:37,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:12:06,976 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:12:06,978 >> Configuration saved in bert-base-ml-ner/checkpoint-1300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:12:08,360 >> Model weights saved in bert-base-ml-ner/checkpoint-1300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:12:18,220 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1100] due to args.save_total_limit\n",
            " 42% 1400/3360 [09:17<11:04,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:12:51,281 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:12:51,282 >> Configuration saved in bert-base-ml-ner/checkpoint-1400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:12:52,726 >> Model weights saved in bert-base-ml-ner/checkpoint-1400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:12:56,341 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0009, 'learning_rate': 1.660714285714286e-05, 'epoch': 71.43}\n",
            " 45% 1500/3360 [09:55<10:27,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:13:29,597 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:13:29,598 >> Configuration saved in bert-base-ml-ner/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:13:30,981 >> Model weights saved in bert-base-ml-ner/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:13:41,661 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1300] due to args.save_total_limit\n",
            " 48% 1600/3360 [10:40<09:34,  3.06it/s][INFO|trainer.py:2815] 2023-10-31 02:14:14,685 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:14:14,687 >> Configuration saved in bert-base-ml-ner/checkpoint-1600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:14:16,275 >> Model weights saved in bert-base-ml-ner/checkpoint-1600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:14:19,718 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1400] due to args.save_total_limit\n",
            " 51% 1700/3360 [11:18<09:30,  2.91it/s][INFO|trainer.py:2815] 2023-10-31 02:14:53,069 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:14:53,071 >> Configuration saved in bert-base-ml-ner/checkpoint-1700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:14:54,633 >> Model weights saved in bert-base-ml-ner/checkpoint-1700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:15:03,240 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1500] due to args.save_total_limit\n",
            " 54% 1800/3360 [12:02<08:49,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:15:36,380 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:15:36,381 >> Configuration saved in bert-base-ml-ner/checkpoint-1800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:15:37,918 >> Model weights saved in bert-base-ml-ner/checkpoint-1800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:15:48,188 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1600] due to args.save_total_limit\n",
            " 57% 1900/3360 [12:47<08:07,  2.99it/s][INFO|trainer.py:2815] 2023-10-31 02:16:21,310 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:16:21,311 >> Configuration saved in bert-base-ml-ner/checkpoint-1900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:16:22,767 >> Model weights saved in bert-base-ml-ner/checkpoint-1900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:16:28,897 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 1.2142857142857144e-05, 'epoch': 95.24}\n",
            " 60% 2000/3360 [13:27<07:30,  3.02it/s][INFO|trainer.py:2815] 2023-10-31 02:17:02,096 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:17:02,097 >> Configuration saved in bert-base-ml-ner/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:17:04,794 >> Model weights saved in bert-base-ml-ner/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:17:08,519 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1800] due to args.save_total_limit\n",
            " 62% 2100/3360 [14:07<05:45,  3.65it/s][INFO|trainer.py:2815] 2023-10-31 02:17:41,746 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:17:41,747 >> Configuration saved in bert-base-ml-ner/checkpoint-2100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:17:43,326 >> Model weights saved in bert-base-ml-ner/checkpoint-2100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:17:54,172 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1900] due to args.save_total_limit\n",
            " 65% 2200/3360 [14:53<06:34,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:18:27,429 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:18:27,430 >> Configuration saved in bert-base-ml-ner/checkpoint-2200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:18:28,898 >> Model weights saved in bert-base-ml-ner/checkpoint-2200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:18:36,144 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2000] due to args.save_total_limit\n",
            " 68% 2300/3360 [15:35<05:55,  2.98it/s][INFO|trainer.py:2815] 2023-10-31 02:19:09,348 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:19:09,349 >> Configuration saved in bert-base-ml-ner/checkpoint-2300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:19:10,863 >> Model weights saved in bert-base-ml-ner/checkpoint-2300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:19:22,074 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2100] due to args.save_total_limit\n",
            " 71% 2400/3360 [16:20<05:21,  2.99it/s][INFO|trainer.py:2815] 2023-10-31 02:19:55,029 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:19:55,031 >> Configuration saved in bert-base-ml-ner/checkpoint-2400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:19:56,559 >> Model weights saved in bert-base-ml-ner/checkpoint-2400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:20:00,068 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2200] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 7.678571428571428e-06, 'epoch': 119.05}\n",
            " 74% 2500/3360 [16:59<04:14,  3.38it/s][INFO|trainer.py:2815] 2023-10-31 02:20:33,392 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:20:33,394 >> Configuration saved in bert-base-ml-ner/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:20:34,961 >> Model weights saved in bert-base-ml-ner/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:20:46,221 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2300] due to args.save_total_limit\n",
            " 77% 2600/3360 [17:45<04:17,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:21:19,372 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:21:19,373 >> Configuration saved in bert-base-ml-ner/checkpoint-2600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:21:20,700 >> Model weights saved in bert-base-ml-ner/checkpoint-2600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:21:28,708 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2400] due to args.save_total_limit\n",
            " 80% 2700/3360 [18:27<03:43,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:22:01,802 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:22:01,803 >> Configuration saved in bert-base-ml-ner/checkpoint-2700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:22:03,168 >> Model weights saved in bert-base-ml-ner/checkpoint-2700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:22:17,431 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2500] due to args.save_total_limit\n",
            " 83% 2800/3360 [19:16<03:08,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:22:50,261 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:22:50,263 >> Configuration saved in bert-base-ml-ner/checkpoint-2800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:22:51,794 >> Model weights saved in bert-base-ml-ner/checkpoint-2800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:22:55,270 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2600] due to args.save_total_limit\n",
            " 86% 2900/3360 [19:54<02:21,  3.25it/s][INFO|trainer.py:2815] 2023-10-31 02:23:28,420 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:23:28,421 >> Configuration saved in bert-base-ml-ner/checkpoint-2900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:23:29,732 >> Model weights saved in bert-base-ml-ner/checkpoint-2900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:23:35,662 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2700] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 3.2142857142857143e-06, 'epoch': 142.86}\n",
            " 89% 3000/3360 [20:34<02:02,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:24:09,042 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:24:09,043 >> Configuration saved in bert-base-ml-ner/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:24:10,462 >> Model weights saved in bert-base-ml-ner/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:24:21,303 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2800] due to args.save_total_limit\n",
            " 92% 3100/3360 [21:20<01:28,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:24:54,373 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:24:54,375 >> Configuration saved in bert-base-ml-ner/checkpoint-3100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:24:55,951 >> Model weights saved in bert-base-ml-ner/checkpoint-3100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:24:59,499 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2900] due to args.save_total_limit\n",
            " 95% 3200/3360 [21:58<00:53,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:25:32,799 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:25:32,801 >> Configuration saved in bert-base-ml-ner/checkpoint-3200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:25:34,270 >> Model weights saved in bert-base-ml-ner/checkpoint-3200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:25:40,562 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3000] due to args.save_total_limit\n",
            " 98% 3300/3360 [22:39<00:19,  3.13it/s][INFO|trainer.py:2815] 2023-10-31 02:26:13,865 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:26:13,867 >> Configuration saved in bert-base-ml-ner/checkpoint-3300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:26:15,486 >> Model weights saved in bert-base-ml-ner/checkpoint-3300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:26:22,757 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3100] due to args.save_total_limit\n",
            "100% 3360/3360 [23:08<00:00,  3.65it/s][INFO|trainer.py:1911] 2023-10-31 02:26:42,792 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1388.6828, 'train_samples_per_second': 37.33, 'train_steps_per_second': 2.42, 'train_loss': 0.0036983033767596465, 'epoch': 160.0}\n",
            "100% 3360/3360 [23:08<00:00,  2.42it/s]\n",
            "[INFO|trainer.py:2815] 2023-10-31 02:26:42,826 >> Saving model checkpoint to bert-base-ml-ner\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:26:42,827 >> Configuration saved in bert-base-ml-ner/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:26:44,248 >> Model weights saved in bert-base-ml-ner/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-10-31 02:26:44,250 >> tokenizer config file saved in bert-base-ml-ner/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-10-31 02:26:44,250 >> Special tokens file saved in bert-base-ml-ner/special_tokens_map.json\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   Creating features from dataset file at ./\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   Writing example 0 of 40\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   guid: test-1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   tokens: [CLS] uno de los objetivos más importantes del proyecto , fue crear un conjunto de rutina ##s independientes del hard ##w ##are , que se auto configurar ##an al tiempo de ejecución de manera transparente al programado ##r sin que por ello se perdi ##era la sencil ##lez de programa ##ci ##6 ##n o el alcance y efectividad de la unidad . cabe mencionar que siempre se tuvo en mente el tratar de proporcionar un ambiente agradable como resultado del uso de las rutina ##s de la unidad , para lo cual se diseñar ##on las presentaciones tomando el mayor cuidado posible , haciendo siempre consultas a diferentes personas a las cuales les estoy agradecido por su valiosa participación [SEP]\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_ids: 4 1614 1009 1067 3501 1186 3521 1081 2269 1019 1247 3959 1044 3877 1009 14619 30958 8751 1081 11722 1004 3992 1019 1041 1057 2183 26175 1029 1074 1526 1009 4242 1009 2160 14797 1074 19313 30960 1320 1041 1076 2601 1057 23696 1178 1032 4378 7678 1009 1952 1024 1000 30959 1068 1039 5909 1040 19651 1009 1032 4205 1008 6324 9691 1041 1772 1057 2839 1035 4260 1039 4221 1009 6776 1044 3508 6495 1151 3776 1081 2889 1009 1085 14619 30958 1009 1032 4205 1019 1097 1086 1596 1057 18043 1022 1085 14211 5794 1039 1736 3234 2368 1019 2194 1772 6412 1012 3055 1845 1012 1085 3523 1777 1435 15793 1076 1069 16598 3374 5 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 2 2 0 1 1 1 1 -100 1 1 1 -100 -100 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   guid: test-2\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   tokens: [CLS] . [SEP]\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_ids: 4 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   guid: test-3\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   tokens: [CLS] en el presente trabajo se pretende : presentar una breve introducción a los protocolos de en ##ru ##tamiento identificar las características deseable ##s en los protocolos de en ##ru ##tamiento realizar un análisis descrip ##tivo de los protocolos más importantes , así como las razones de su éxito mediante una simulación en un programa de entrenamiento el mecanismo que se lleva a cabo para configurar en un en ##ru ##tador un protocolo de en ##ru ##tamiento simple , como lo es el protocolo [SEP]\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_ids: 4 1035 1039 2807 1608 1057 10185 995 3729 1091 7178 6760 1012 1067 11902 1009 1035 1366 10765 8849 1085 5616 27280 30958 1035 1067 11902 1009 1035 1366 10765 4267 1044 4089 13429 1483 1009 1067 11902 1186 3521 1019 1337 1151 1085 5367 1009 1069 3613 2811 1091 22327 1035 1044 1952 1009 8222 1039 6267 1041 1057 3664 1012 3170 1097 26175 1035 1044 1035 1366 4633 1044 4584 1009 1035 1366 10765 2940 1019 1151 1086 1028 1039 4584 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 -100 2 2 2 2 2 -100 -100 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   guid: test-4\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   tokens: [CLS] nuestro objetivo principal es crear claus ##ulas multi ##valor ##es que mejore ##n la expres ##ividad del lenguaje en il ##p , con ello se espera que los algoritmo ##s il ##p constru ##yan hipo ##tesis con menos claus ##ulas y por lo tanto mas facil ##es de interpretar , utilizando los algoritmo ##s de lenguaje il ##p [SEP]\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_ids: 4 2045 3073 3025 1028 3959 8933 4955 4284 27882 1018 1041 25635 30959 1032 2749 18582 1081 8023 1035 4070 30968 1019 1048 2601 1057 2216 1041 1067 23497 30958 4070 30968 3054 3516 8484 7703 1048 1885 8933 4955 1040 1076 1086 1850 2062 3476 1018 1009 13146 1019 7553 1067 23497 30958 1009 8023 4070 30968 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   label_ids: -100 2 2 2 2 0 1 -100 1 -100 -100 1 1 -100 1 1 -100 1 1 1 1 -100 2 2 2 2 2 2 2 2 -100 2 -100 2 -100 2 -100 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   guid: test-5\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   tokens: [CLS] observar la evolución del comportamiento de los nodos de un sistema p ##2 ##p , cuando reciben incentivos por su coopera ##cion ( duplic ##ar ) , para lo cual model ##amos la situación estratégica de los nodos ( duplic ##ar , no duplic ##ar ) y el sistema con un juego , utilizando mecanismos que compens ##en la cooperación de los nodos . [SEP]\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_ids: 4 9069 1032 6066 1081 7060 1009 1067 30926 1009 1044 2074 1011 30989 30968 1019 1351 9868 17436 1076 1069 9356 1105 1147 12452 1020 1135 1019 1097 1086 1596 26385 1209 1032 2471 12241 1009 1067 30926 1147 12452 1020 1019 1054 12452 1020 1135 1040 1039 2074 1048 1044 2934 1019 7553 5719 1041 7840 1017 1032 2730 1009 1067 30926 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 2 2 2 -100 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 -100 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:26:44 - INFO - utils_ner -   Saving features into cached file ./cached_test_BertTokenizer_128\n",
            "[INFO|trainer.py:3092] 2023-10-31 02:26:44,421 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3094] 2023-10-31 02:26:44,422 >>   Num examples = 40\n",
            "[INFO|trainer.py:3097] 2023-10-31 02:26:44,422 >>   Batch size = 8\n",
            "100% 5/5 [00:00<00:00, 15.16it/s]\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_loss = 1.0883427858352661\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_accuracy_score = 0.8703962703962704\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_precision = 0.2222222222222222\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_recall = 0.3333333333333333\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_f1 = 0.26666666666666666\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_runtime = 0.385\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_samples_per_second = 103.883\n",
            "10/31/2023 02:26:44 - INFO - __main__ -     test_steps_per_second = 12.985\n",
            "/bin/bash: line 2: .json: command not found\n",
            "/bin/bash: line 2: .txt: command not found\n",
            "/bin/bash: line 2: .txt: command not found\n",
            "test_accuracy_score 0.8703962703962704 epoch_160_learn_3e-05_batch_16_corpusArgPC_fold0\n",
            "\n",
            "test_f1 0.26666666666666666 epoch_160_learn_3e-05_batch_16_corpusArgPC_fold0\n",
            "\n",
            "RESULTADOS FOLDER 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-QUE       0.85      0.83      0.84        35\n",
            "       I-QUE       0.76      0.55      0.64       431\n",
            "           O       0.89      0.95      0.92      1631\n",
            "\n",
            "    accuracy                           0.87      2097\n",
            "   macro avg       0.83      0.78      0.80      2097\n",
            "weighted avg       0.86      0.87      0.86      2097\n",
            "\n",
            "2023-10-31 02:27:08.417741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-31 02:27:08.417801: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-31 02:27:08.417843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-31 02:27:09.929352: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/31/2023 02:27:13 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "10/31/2023 02:27:13 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-base-ml-ner/runs/Oct31_02-27-13_3952901ecc4a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=160.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=bert-base-ml-ner,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-base-ml-ner,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:27:13,309 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:27:13,316 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-QUE\",\n",
            "    \"1\": \"I-QUE\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-QUE\": 0,\n",
            "    \"I-QUE\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:27:13,377 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:27:13,378 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:27:13,379 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:27:13,379 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:27:13,379 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:27:13,380 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:27:13,380 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer.json\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:27:13,380 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:27:13,381 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3083] 2023-10-31 02:27:13,575 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3861] 2023-10-31 02:27:17,706 >> Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3873] 2023-10-31 02:27:17,706 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   Creating features from dataset file at ./\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   Writing example 0 of 326\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   guid: train-1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   tokens: [CLS] uno de los objetivos más importantes del proyecto , fue crear un conjunto de rutina ##s independientes del hard ##w ##are , que se auto configurar ##an al tiempo de ejecución de manera transparente al programado ##r sin que por ello se perdi ##era la sencil ##lez de programa ##ci ##6 ##n o el alcance y efectividad de la unidad . cabe mencionar que siempre se tuvo en mente el tratar de proporcionar un ambiente agradable como resultado del uso de las rutina ##s de la unidad , para lo cual se diseñar ##on las presentaciones tomando el mayor cuidado posible , haciendo siempre consultas a diferentes personas a las cuales les estoy agradecido por su valiosa participación . [SEP]\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_ids: 4 1614 1009 1067 3501 1186 3521 1081 2269 1019 1247 3959 1044 3877 1009 14619 30958 8751 1081 11722 1004 3992 1019 1041 1057 2183 26175 1029 1074 1526 1009 4242 1009 2160 14797 1074 19313 30960 1320 1041 1076 2601 1057 23696 1178 1032 4378 7678 1009 1952 1024 1000 30959 1068 1039 5909 1040 19651 1009 1032 4205 1008 6324 9691 1041 1772 1057 2839 1035 4260 1039 4221 1009 6776 1044 3508 6495 1151 3776 1081 2889 1009 1085 14619 30958 1009 1032 4205 1019 1097 1086 1596 1057 18043 1022 1085 14211 5794 1039 1736 3234 2368 1019 2194 1772 6412 1012 3055 1845 1012 1085 3523 1777 1435 15793 1076 1069 16598 3374 1008 5 1 1 1 1 1 1 1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 2 2 0 1 1 1 1 -100 1 1 1 -100 -100 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   guid: train-2\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   tokens: [CLS] en el presente trabajo se pretende : presentar una breve introducción a los protocolos de en ##ru ##tamiento identificar las características deseable ##s en los protocolos de en ##ru ##tamiento realizar un análisis descrip ##tivo de los protocolos más importantes , así como las razones de su éxito mediante una simulación en un programa de entrenamiento el mecanismo que se lleva a cabo para configurar en un en ##ru ##tador un protocolo de en ##ru ##tamiento simple , como lo es el protocolo [SEP]\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_ids: 4 1035 1039 2807 1608 1057 10185 995 3729 1091 7178 6760 1012 1067 11902 1009 1035 1366 10765 8849 1085 5616 27280 30958 1035 1067 11902 1009 1035 1366 10765 4267 1044 4089 13429 1483 1009 1067 11902 1186 3521 1019 1337 1151 1085 5367 1009 1069 3613 2811 1091 22327 1035 1044 1952 1009 8222 1039 6267 1041 1057 3664 1012 3170 1097 26175 1035 1044 1035 1366 4633 1044 4584 1009 1035 1366 10765 2940 1019 1151 1086 1028 1039 4584 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 -100 2 2 2 2 2 -100 -100 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   guid: train-3\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   tokens: [CLS] nuestro objetivo principal es crear claus ##ulas multi ##valor ##es que mejore ##n la expres ##ividad del lenguaje en il ##p , con ello se espera que los algoritmo ##s il ##p constru ##yan hipo ##tesis con menos claus ##ulas y por lo tanto mas facil ##es de interpretar , utilizando los algoritmo ##s de lenguaje il ##p [SEP]\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_ids: 4 2045 3073 3025 1028 3959 8933 4955 4284 27882 1018 1041 25635 30959 1032 2749 18582 1081 8023 1035 4070 30968 1019 1048 2601 1057 2216 1041 1067 23497 30958 4070 30968 3054 3516 8484 7703 1048 1885 8933 4955 1040 1076 1086 1850 2062 3476 1018 1009 13146 1019 7553 1067 23497 30958 1009 8023 4070 30968 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   label_ids: -100 2 2 2 2 0 1 -100 1 -100 -100 1 1 -100 1 1 -100 1 1 1 1 -100 2 2 2 2 2 2 2 2 -100 2 -100 2 -100 2 -100 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   guid: train-4\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   tokens: [CLS] observar la evolución del comportamiento de los nodos de un sistema p ##2 ##p , cuando reciben incentivos por su coopera ##cion ( duplic ##ar ) , para lo cual model ##amos la situación estratégica de los nodos ( duplic ##ar , no duplic ##ar ) y el sistema con un juego , utilizando mecanismos que compens ##en la cooperación de los nodos . [SEP]\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_ids: 4 9069 1032 6066 1081 7060 1009 1067 30926 1009 1044 2074 1011 30989 30968 1019 1351 9868 17436 1076 1069 9356 1105 1147 12452 1020 1135 1019 1097 1086 1596 26385 1209 1032 2471 12241 1009 1067 30926 1147 12452 1020 1019 1054 12452 1020 1135 1040 1039 2074 1048 1044 2934 1019 7553 5719 1041 7840 1017 1032 2730 1009 1067 30926 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 2 2 2 -100 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 -100 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   guid: train-5\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   tokens: [CLS] el objetivo al elaborar esta aplicación es el introducir ##se al desarrollo de soft ##w ##are , así como conocer y aprender a programar en el ambiente orientado a objetos en este caso en visual c + + utilizando un lenguaje para computación móvil . [SEP]\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_ids: 4 1039 3073 1074 8445 1149 2407 1028 1039 10219 1182 1074 1766 1009 7193 1004 3992 1019 1337 1151 3266 1040 5331 1012 26817 1035 1039 3508 19456 1012 6997 1035 1277 2053 1035 11606 1013 967 967 7553 1044 8023 1097 25466 6803 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:27:17 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 -100 1 1 1 1 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:27:19 - INFO - utils_ner -   Saving features into cached file ./cached_train_BertTokenizer_128\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:27:27,691 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1454: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:27:27,692 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1679] 2023-10-31 02:27:27,949 >> ***** Running training *****\n",
            "[INFO|trainer.py:1680] 2023-10-31 02:27:27,949 >>   Num examples = 326\n",
            "[INFO|trainer.py:1681] 2023-10-31 02:27:27,949 >>   Num Epochs = 160\n",
            "[INFO|trainer.py:1682] 2023-10-31 02:27:27,949 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1684] 2023-10-31 02:27:27,949 >>   Training with DataParallel so batch size has been adjusted to: 16\n",
            "[INFO|trainer.py:1685] 2023-10-31 02:27:27,949 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1686] 2023-10-31 02:27:27,949 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1687] 2023-10-31 02:27:27,949 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1688] 2023-10-31 02:27:27,950 >>   Number of trainable parameters = 109,262,595\n",
            "  3% 100/3360 [00:32<17:48,  3.05it/s][INFO|trainer.py:2815] 2023-10-31 02:28:00,672 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:28:00,673 >> Configuration saved in bert-base-ml-ner/checkpoint-100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:28:02,240 >> Model weights saved in bert-base-ml-ner/checkpoint-100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:28:15,154 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3200] due to args.save_total_limit\n",
            "  6% 200/3360 [01:19<17:24,  3.03it/s][INFO|trainer.py:2815] 2023-10-31 02:28:47,352 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:28:47,353 >> Configuration saved in bert-base-ml-ner/checkpoint-200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:28:48,847 >> Model weights saved in bert-base-ml-ner/checkpoint-200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:28:58,895 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3300] due to args.save_total_limit\n",
            "  9% 300/3360 [02:03<16:54,  3.02it/s][INFO|trainer.py:2815] 2023-10-31 02:29:31,426 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:29:31,428 >> Configuration saved in bert-base-ml-ner/checkpoint-300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:29:32,834 >> Model weights saved in bert-base-ml-ner/checkpoint-300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:29:40,959 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-100] due to args.save_total_limit\n",
            " 12% 400/3360 [02:45<14:59,  3.29it/s][INFO|trainer.py:2815] 2023-10-31 02:30:13,908 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:30:13,909 >> Configuration saved in bert-base-ml-ner/checkpoint-400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:30:15,515 >> Model weights saved in bert-base-ml-ner/checkpoint-400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:30:25,534 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.0288, 'learning_rate': 2.5535714285714284e-05, 'epoch': 23.81}\n",
            " 15% 500/3360 [03:30<16:05,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:30:58,747 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:30:58,748 >> Configuration saved in bert-base-ml-ner/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:31:00,066 >> Model weights saved in bert-base-ml-ner/checkpoint-500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:31:03,730 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-300] due to args.save_total_limit\n",
            " 18% 600/3360 [04:09<15:34,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:31:37,011 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:31:37,012 >> Configuration saved in bert-base-ml-ner/checkpoint-600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:31:38,393 >> Model weights saved in bert-base-ml-ner/checkpoint-600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:31:49,403 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-400] due to args.save_total_limit\n",
            " 21% 700/3360 [04:54<14:51,  2.99it/s][INFO|trainer.py:2815] 2023-10-31 02:32:22,636 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:32:22,637 >> Configuration saved in bert-base-ml-ner/checkpoint-700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:32:24,018 >> Model weights saved in bert-base-ml-ner/checkpoint-700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:32:27,588 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-500] due to args.save_total_limit\n",
            " 24% 800/3360 [05:33<13:23,  3.18it/s][INFO|trainer.py:2815] 2023-10-31 02:33:01,063 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:33:01,064 >> Configuration saved in bert-base-ml-ner/checkpoint-800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:33:02,433 >> Model weights saved in bert-base-ml-ner/checkpoint-800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:33:08,956 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-600] due to args.save_total_limit\n",
            " 27% 900/3360 [06:14<14:01,  2.92it/s][INFO|trainer.py:2815] 2023-10-31 02:33:42,512 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:33:42,514 >> Configuration saved in bert-base-ml-ner/checkpoint-900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:33:44,046 >> Model weights saved in bert-base-ml-ner/checkpoint-900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:33:49,934 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0006, 'learning_rate': 2.107142857142857e-05, 'epoch': 47.62}\n",
            " 30% 1000/3360 [06:55<13:30,  2.91it/s][INFO|trainer.py:2815] 2023-10-31 02:34:23,430 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:34:23,432 >> Configuration saved in bert-base-ml-ner/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:34:25,040 >> Model weights saved in bert-base-ml-ner/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:34:34,368 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-800] due to args.save_total_limit\n",
            " 33% 1100/3360 [07:39<12:43,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:35:07,685 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:35:07,686 >> Configuration saved in bert-base-ml-ner/checkpoint-1100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:35:09,069 >> Model weights saved in bert-base-ml-ner/checkpoint-1100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:35:12,651 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-900] due to args.save_total_limit\n",
            " 36% 1200/3360 [08:18<11:40,  3.08it/s][INFO|trainer.py:2815] 2023-10-31 02:35:46,112 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:35:46,113 >> Configuration saved in bert-base-ml-ner/checkpoint-1200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:35:47,457 >> Model weights saved in bert-base-ml-ner/checkpoint-1200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:35:57,465 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1000] due to args.save_total_limit\n",
            " 39% 1300/3360 [09:03<11:43,  2.93it/s][INFO|trainer.py:2815] 2023-10-31 02:36:31,048 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:36:31,049 >> Configuration saved in bert-base-ml-ner/checkpoint-1300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:36:32,750 >> Model weights saved in bert-base-ml-ner/checkpoint-1300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:36:39,419 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1100] due to args.save_total_limit\n",
            " 42% 1400/3360 [09:44<11:09,  2.93it/s][INFO|trainer.py:2815] 2023-10-31 02:37:12,882 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:37:12,884 >> Configuration saved in bert-base-ml-ner/checkpoint-1400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:37:14,324 >> Model weights saved in bert-base-ml-ner/checkpoint-1400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:37:23,375 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 1.660714285714286e-05, 'epoch': 71.43}\n",
            " 45% 1500/3360 [10:28<10:35,  2.93it/s][INFO|trainer.py:2815] 2023-10-31 02:37:56,724 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:37:56,726 >> Configuration saved in bert-base-ml-ner/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:37:58,372 >> Model weights saved in bert-base-ml-ner/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:38:04,732 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1300] due to args.save_total_limit\n",
            " 48% 1600/3360 [11:10<09:35,  3.06it/s][INFO|trainer.py:2815] 2023-10-31 02:38:38,079 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:38:38,080 >> Configuration saved in bert-base-ml-ner/checkpoint-1600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:38:39,533 >> Model weights saved in bert-base-ml-ner/checkpoint-1600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:38:47,730 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1400] due to args.save_total_limit\n",
            " 51% 1700/3360 [11:53<09:25,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:39:21,387 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:39:21,388 >> Configuration saved in bert-base-ml-ner/checkpoint-1700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:39:22,824 >> Model weights saved in bert-base-ml-ner/checkpoint-1700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:39:32,917 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1500] due to args.save_total_limit\n",
            " 54% 1800/3360 [12:38<08:54,  2.92it/s][INFO|trainer.py:2815] 2023-10-31 02:40:06,176 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:40:06,178 >> Configuration saved in bert-base-ml-ner/checkpoint-1800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:40:07,706 >> Model weights saved in bert-base-ml-ner/checkpoint-1800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:40:11,201 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1600] due to args.save_total_limit\n",
            " 57% 1900/3360 [13:16<08:20,  2.92it/s][INFO|trainer.py:2815] 2023-10-31 02:40:44,717 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:40:44,719 >> Configuration saved in bert-base-ml-ner/checkpoint-1900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:40:46,244 >> Model weights saved in bert-base-ml-ner/checkpoint-1900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:40:49,708 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1700] due to args.save_total_limit\n",
            "{'loss': 0.0005, 'learning_rate': 1.2142857142857144e-05, 'epoch': 95.24}\n",
            " 60% 2000/3360 [13:55<07:34,  2.99it/s][INFO|trainer.py:2815] 2023-10-31 02:41:23,254 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:41:23,255 >> Configuration saved in bert-base-ml-ner/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:41:24,820 >> Model weights saved in bert-base-ml-ner/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:41:37,242 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1800] due to args.save_total_limit\n",
            " 62% 2100/3360 [14:42<05:58,  3.51it/s][INFO|trainer.py:2815] 2023-10-31 02:42:10,528 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:42:10,529 >> Configuration saved in bert-base-ml-ner/checkpoint-2100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:42:11,838 >> Model weights saved in bert-base-ml-ner/checkpoint-2100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:42:19,478 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1900] due to args.save_total_limit\n",
            " 65% 2200/3360 [15:25<06:34,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:42:53,047 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:42:53,048 >> Configuration saved in bert-base-ml-ner/checkpoint-2200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:42:54,504 >> Model weights saved in bert-base-ml-ner/checkpoint-2200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:42:57,989 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2000] due to args.save_total_limit\n",
            " 68% 2300/3360 [16:03<06:00,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:43:31,570 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:43:31,571 >> Configuration saved in bert-base-ml-ner/checkpoint-2300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:43:32,892 >> Model weights saved in bert-base-ml-ner/checkpoint-2300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:43:40,989 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2100] due to args.save_total_limit\n",
            " 71% 2400/3360 [16:46<05:24,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:44:14,438 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:44:14,440 >> Configuration saved in bert-base-ml-ner/checkpoint-2400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:44:15,991 >> Model weights saved in bert-base-ml-ner/checkpoint-2400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:44:32,327 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2200] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 7.678571428571428e-06, 'epoch': 119.05}\n",
            " 74% 2500/3360 [17:37<04:17,  3.35it/s][INFO|trainer.py:2815] 2023-10-31 02:45:05,511 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:45:05,512 >> Configuration saved in bert-base-ml-ner/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:45:06,910 >> Model weights saved in bert-base-ml-ner/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:45:19,551 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2300] due to args.save_total_limit\n",
            " 77% 2600/3360 [18:24<04:19,  2.93it/s][INFO|trainer.py:2815] 2023-10-31 02:45:52,819 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:45:52,821 >> Configuration saved in bert-base-ml-ner/checkpoint-2600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:45:54,385 >> Model weights saved in bert-base-ml-ner/checkpoint-2600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:46:05,708 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2400] due to args.save_total_limit\n",
            " 80% 2700/3360 [19:11<03:42,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:46:39,197 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:46:39,198 >> Configuration saved in bert-base-ml-ner/checkpoint-2700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:46:40,660 >> Model weights saved in bert-base-ml-ner/checkpoint-2700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:46:48,026 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2500] due to args.save_total_limit\n",
            " 83% 2800/3360 [19:53<03:08,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:47:21,295 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:47:21,296 >> Configuration saved in bert-base-ml-ner/checkpoint-2800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:47:22,687 >> Model weights saved in bert-base-ml-ner/checkpoint-2800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:47:26,214 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2600] due to args.save_total_limit\n",
            " 86% 2900/3360 [20:31<02:24,  3.19it/s][INFO|trainer.py:2815] 2023-10-31 02:47:59,837 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-2900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:47:59,838 >> Configuration saved in bert-base-ml-ner/checkpoint-2900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:48:01,288 >> Model weights saved in bert-base-ml-ner/checkpoint-2900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:48:07,998 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2700] due to args.save_total_limit\n",
            "{'loss': 0.0001, 'learning_rate': 3.2142857142857143e-06, 'epoch': 142.86}\n",
            " 89% 3000/3360 [21:13<02:03,  2.91it/s][INFO|trainer.py:2815] 2023-10-31 02:48:41,633 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:48:41,635 >> Configuration saved in bert-base-ml-ner/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:48:43,176 >> Model weights saved in bert-base-ml-ner/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:48:49,990 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2800] due to args.save_total_limit\n",
            " 92% 3100/3360 [21:55<01:29,  2.89it/s][INFO|trainer.py:2815] 2023-10-31 02:49:23,472 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:49:23,473 >> Configuration saved in bert-base-ml-ner/checkpoint-3100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:49:25,047 >> Model weights saved in bert-base-ml-ner/checkpoint-3100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:49:35,059 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-2900] due to args.save_total_limit\n",
            " 95% 3200/3360 [22:40<00:54,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 02:50:08,421 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:50:08,422 >> Configuration saved in bert-base-ml-ner/checkpoint-3200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:50:09,898 >> Model weights saved in bert-base-ml-ner/checkpoint-3200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:50:13,449 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3000] due to args.save_total_limit\n",
            " 98% 3300/3360 [23:18<00:19,  3.10it/s][INFO|trainer.py:2815] 2023-10-31 02:50:46,934 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-3300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:50:46,935 >> Configuration saved in bert-base-ml-ner/checkpoint-3300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:50:48,347 >> Model weights saved in bert-base-ml-ner/checkpoint-3300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:51:02,898 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3100] due to args.save_total_limit\n",
            "100% 3360/3360 [23:54<00:00,  3.54it/s][INFO|trainer.py:1911] 2023-10-31 02:51:22,818 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1434.8995, 'train_samples_per_second': 36.351, 'train_steps_per_second': 2.342, 'train_loss': 0.0044743592918079925, 'epoch': 160.0}\n",
            "100% 3360/3360 [23:54<00:00,  2.34it/s]\n",
            "[INFO|trainer.py:2815] 2023-10-31 02:51:22,851 >> Saving model checkpoint to bert-base-ml-ner\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:51:22,852 >> Configuration saved in bert-base-ml-ner/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:51:24,270 >> Model weights saved in bert-base-ml-ner/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2428] 2023-10-31 02:51:24,272 >> tokenizer config file saved in bert-base-ml-ner/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2437] 2023-10-31 02:51:24,272 >> Special tokens file saved in bert-base-ml-ner/special_tokens_map.json\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   Creating features from dataset file at ./\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   Writing example 0 of 36\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   guid: test-1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   tokens: [CLS] el objetivo del presente trabajo es estudiar y analizar la viabilidad de la implementación de un entre ##laz ##ado en tiempo de ejecución sobre la máquina virtual del proyecto mono ademas se estudiar ##á el impacto en el tiempo de carga y ejecución de la implementación utilizando una arquitectura escalo ##nada de experimenta ##cion [SEP]\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_ids: 4 1039 3073 1081 2807 1608 1028 6661 1040 10190 1032 18137 1009 1032 13811 1009 1044 1341 4424 1082 1035 1526 1009 4242 1246 1032 6055 15417 1081 2269 6172 27424 1057 6661 30980 1039 6501 1035 1039 1526 1009 5260 1040 4242 1009 1032 13811 7553 1091 8524 20977 20143 1009 20451 1105 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 1 1 -100 -100 1 1 1 1 1 1 1 1 1 1 1 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   guid: test-2\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   tokens: [CLS] determinar la influencia de la implementación de un sistema de información basado en un enfoque de procesos en la opera ##tividad del área de créditos de la micro ##finan ##ciera crecer bajo una plataforma orientada a la w ##eb [SEP]\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_ids: 4 5679 1032 6135 1009 1032 13811 1009 1044 2074 1009 1926 6588 1035 1044 5877 1009 5676 1035 1032 2969 4767 1081 3967 1009 7701 1009 1032 6318 16796 15182 9457 2182 1091 7688 19515 1012 1032 1005 3686 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   guid: test-3\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   tokens: [CLS] desarrollar una red de área local ( lan ) que facilite la comunicación en el centro local amazonas de la universidad nacional abierta . [SEP]\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_ids: 4 5201 1091 2946 1009 3967 3592 1147 2428 1135 1041 25162 1032 3617 1035 1039 2391 3592 21281 1009 1032 2862 1954 6051 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   guid: test-4\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   tokens: [CLS] diseñar e implementar , un modelo de precisión para detectar y mitigar ph ##ishi ##ng en correos electrónicos utilizando técnicas actuales de minería de datos para aumentar el nivel de seguridad de los correos electrónicos . [SEP]\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_ids: 4 18043 1006 19118 1019 1044 4209 1009 11651 1097 13666 1040 22315 4398 29742 22396 1035 14256 12809 7553 5967 6966 1009 18596 1009 2783 1097 4928 1039 2174 1009 1955 1009 1067 14256 12809 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   guid: test-5\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   tokens: [CLS] desarrollar un sistema de monitoreo basado en la tecnología de comunicación lor ##a para determinar la calidad del agua dulce que se consume en zonas rurales [SEP]\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_ids: 4 5201 1044 2074 1009 24624 6588 1035 1032 3925 1009 3617 6756 30956 1097 5679 1032 3339 1081 2326 5525 1041 1057 23893 1035 3762 8186 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:24 - INFO - utils_ner -   Saving features into cached file ./cached_test_BertTokenizer_128\n",
            "[INFO|trainer.py:3092] 2023-10-31 02:51:24,391 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:3094] 2023-10-31 02:51:24,391 >>   Num examples = 36\n",
            "[INFO|trainer.py:3097] 2023-10-31 02:51:24,391 >>   Batch size = 8\n",
            "100% 5/5 [00:00<00:00, 18.06it/s]\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_loss = 0.4812995195388794\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_accuracy_score = 0.9434502505368647\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_precision = 0.6052631578947368\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_recall = 0.6388888888888888\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_f1 = 0.6216216216216216\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_runtime = 0.3327\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_samples_per_second = 108.207\n",
            "10/31/2023 02:51:24 - INFO - __main__ -     test_steps_per_second = 15.029\n",
            "/bin/bash: line 2: .json: command not found\n",
            "/bin/bash: line 2: .txt: command not found\n",
            "/bin/bash: line 2: .txt: command not found\n",
            "test_accuracy_score 0.9434502505368647 epoch_160_learn_3e-05_batch_16_corpusArgPC_fold1\n",
            "\n",
            "test_f1 0.6216216216216216 epoch_160_learn_3e-05_batch_16_corpusArgPC_fold1\n",
            "\n",
            "RESULTADOS FOLDER 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-QUE       0.97      0.97      0.97        35\n",
            "       I-QUE       0.88      0.82      0.85       267\n",
            "           O       0.96      0.97      0.96      1047\n",
            "\n",
            "    accuracy                           0.94      1349\n",
            "   macro avg       0.93      0.92      0.93      1349\n",
            "weighted avg       0.94      0.94      0.94      1349\n",
            "\n",
            "2023-10-31 02:51:47.743807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-31 02:51:47.743864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-31 02:51:47.743911: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-31 02:51:50.060830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/31/2023 02:51:53 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
            "10/31/2023 02:51:53 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=bert-base-ml-ner/runs/Oct31_02-51-53_3952901ecc4a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=160.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=bert-base-ml-ner,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=bert-base-ml-ner,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:51:53,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:51:53,896 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-QUE\",\n",
            "    \"1\": \"I-QUE\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-QUE\": 0,\n",
            "    \"I-QUE\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:51:53,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:51:53,951 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:51:53,952 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:51:53,952 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:51:53,952 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:51:53,952 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2022] 2023-10-31 02:51:53,952 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/tokenizer.json\n",
            "[INFO|configuration_utils.py:717] 2023-10-31 02:51:53,952 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/config.json\n",
            "[INFO|configuration_utils.py:777] 2023-10-31 02:51:53,953 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.35.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 31002\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3083] 2023-10-31 02:51:54,138 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--dccuchile--bert-base-spanish-wwm-uncased/snapshots/767afcc9ffdf900341128e9e0bfe44d522461c51/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3861] 2023-10-31 02:51:57,715 >> Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3873] 2023-10-31 02:51:57,715 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   Creating features from dataset file at ./\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   Writing example 0 of 326\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   guid: train-1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   tokens: [CLS] uno de los objetivos más importantes del proyecto , fue crear un conjunto de rutina ##s independientes del hard ##w ##are , que se auto configurar ##an al tiempo de ejecución de manera transparente al programado ##r sin que por ello se perdi ##era la sencil ##lez de programa ##ci ##6 ##n o el alcance y efectividad de la unidad . cabe mencionar que siempre se tuvo en mente el tratar de proporcionar un ambiente agradable como resultado del uso de las rutina ##s de la unidad , para lo cual se diseñar ##on las presentaciones tomando el mayor cuidado posible , haciendo siempre consultas a diferentes personas a las cuales les estoy agradecido por su valiosa participación . [SEP]\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_ids: 4 1614 1009 1067 3501 1186 3521 1081 2269 1019 1247 3959 1044 3877 1009 14619 30958 8751 1081 11722 1004 3992 1019 1041 1057 2183 26175 1029 1074 1526 1009 4242 1009 2160 14797 1074 19313 30960 1320 1041 1076 2601 1057 23696 1178 1032 4378 7678 1009 1952 1024 1000 30959 1068 1039 5909 1040 19651 1009 1032 4205 1008 6324 9691 1041 1772 1057 2839 1035 4260 1039 4221 1009 6776 1044 3508 6495 1151 3776 1081 2889 1009 1085 14619 30958 1009 1032 4205 1019 1097 1086 1596 1057 18043 1022 1085 14211 5794 1039 1736 3234 2368 1019 2194 1772 6412 1012 3055 1845 1012 1085 3523 1777 1435 15793 1076 1069 16598 3374 1008 5 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 2 2 2 0 1 1 1 1 -100 1 1 1 -100 -100 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 -100 2 2 -100 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   guid: train-2\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   tokens: [CLS] en el presente trabajo se pretende : presentar una breve introducción a los protocolos de en ##ru ##tamiento identificar las características deseable ##s en los protocolos de en ##ru ##tamiento realizar un análisis descrip ##tivo de los protocolos más importantes , así como las razones de su éxito mediante una simulación en un programa de entrenamiento el mecanismo que se lleva a cabo para configurar en un en ##ru ##tador un protocolo de en ##ru ##tamiento simple , como lo es el protocolo [SEP]\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_ids: 4 1035 1039 2807 1608 1057 10185 995 3729 1091 7178 6760 1012 1067 11902 1009 1035 1366 10765 8849 1085 5616 27280 30958 1035 1067 11902 1009 1035 1366 10765 4267 1044 4089 13429 1483 1009 1067 11902 1186 3521 1019 1337 1151 1085 5367 1009 1069 3613 2811 1091 22327 1035 1044 1952 1009 8222 1039 6267 1041 1057 3664 1012 3170 1097 26175 1035 1044 1035 1366 4633 1044 4584 1009 1035 1366 10765 2940 1019 1151 1086 1028 1039 4584 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   label_ids: -100 2 2 2 2 2 2 2 0 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 -100 2 2 2 2 2 -100 -100 2 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   guid: train-3\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   tokens: [CLS] nuestro objetivo principal es crear claus ##ulas multi ##valor ##es que mejore ##n la expres ##ividad del lenguaje en il ##p , con ello se espera que los algoritmo ##s il ##p constru ##yan hipo ##tesis con menos claus ##ulas y por lo tanto mas facil ##es de interpretar , utilizando los algoritmo ##s de lenguaje il ##p [SEP]\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_ids: 4 2045 3073 3025 1028 3959 8933 4955 4284 27882 1018 1041 25635 30959 1032 2749 18582 1081 8023 1035 4070 30968 1019 1048 2601 1057 2216 1041 1067 23497 30958 4070 30968 3054 3516 8484 7703 1048 1885 8933 4955 1040 1076 1086 1850 2062 3476 1018 1009 13146 1019 7553 1067 23497 30958 1009 8023 4070 30968 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   label_ids: -100 2 2 2 2 0 1 -100 1 -100 -100 1 1 -100 1 1 -100 1 1 1 1 -100 2 2 2 2 2 2 2 2 -100 2 -100 2 -100 2 -100 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   guid: train-4\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   tokens: [CLS] observar la evolución del comportamiento de los nodos de un sistema p ##2 ##p , cuando reciben incentivos por su coopera ##cion ( duplic ##ar ) , para lo cual model ##amos la situación estratégica de los nodos ( duplic ##ar , no duplic ##ar ) y el sistema con un juego , utilizando mecanismos que compens ##en la cooperación de los nodos . [SEP]\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_ids: 4 9069 1032 6066 1081 7060 1009 1067 30926 1009 1044 2074 1011 30989 30968 1019 1351 9868 17436 1076 1069 9356 1105 1147 12452 1020 1135 1019 1097 1086 1596 26385 1209 1032 2471 12241 1009 1067 30926 1147 12452 1020 1019 1054 12452 1020 1135 1040 1039 2074 1048 1044 2934 1019 7553 5719 1041 7840 1017 1032 2730 1009 1067 30926 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 1 1 1 -100 -100 2 2 2 2 2 2 2 -100 2 2 -100 2 2 2 2 2 2 -100 2 2 2 2 2 2 2 2 -100 2 2 2 -100 2 2 2 2 2 2 2 2 2 2 2 2 -100 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   *** Example ***\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   guid: train-5\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   tokens: [CLS] el objetivo al elaborar esta aplicación es el introducir ##se al desarrollo de soft ##w ##are , así como conocer y aprender a programar en el ambiente orientado a objetos en este caso en visual c + + utilizando un lenguaje para computación móvil . [SEP]\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_ids: 4 1039 3073 1074 8445 1149 2407 1028 1039 10219 1182 1074 1766 1009 7193 1004 3992 1019 1337 1151 3266 1040 5331 1012 26817 1035 1039 3508 19456 1012 6997 1035 1277 2053 1035 11606 1013 967 967 7553 1044 8023 1097 25466 6803 1008 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/31/2023 02:51:57 - INFO - utils_ner -   label_ids: -100 0 1 1 1 1 1 1 1 1 -100 1 1 1 1 -100 -100 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 -100 -100 2 2 2 2 2 2 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "10/31/2023 02:51:58 - INFO - utils_ner -   Saving features into cached file ./cached_train_BertTokenizer_128\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:52:06,967 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1454: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  warnings.warn(\n",
            "[WARNING|training_args.py:1723] 2023-10-31 02:52:06,968 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:1679] 2023-10-31 02:52:07,293 >> ***** Running training *****\n",
            "[INFO|trainer.py:1680] 2023-10-31 02:52:07,294 >>   Num examples = 326\n",
            "[INFO|trainer.py:1681] 2023-10-31 02:52:07,294 >>   Num Epochs = 160\n",
            "[INFO|trainer.py:1682] 2023-10-31 02:52:07,294 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1684] 2023-10-31 02:52:07,294 >>   Training with DataParallel so batch size has been adjusted to: 16\n",
            "[INFO|trainer.py:1685] 2023-10-31 02:52:07,294 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1686] 2023-10-31 02:52:07,294 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1687] 2023-10-31 02:52:07,294 >>   Total optimization steps = 3,360\n",
            "[INFO|trainer.py:1688] 2023-10-31 02:52:07,295 >>   Number of trainable parameters = 109,262,595\n",
            "  3% 100/3360 [00:32<17:49,  3.05it/s][INFO|trainer.py:2815] 2023-10-31 02:52:40,161 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:52:40,162 >> Configuration saved in bert-base-ml-ner/checkpoint-100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:52:41,491 >> Model weights saved in bert-base-ml-ner/checkpoint-100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:52:47,385 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3200] due to args.save_total_limit\n",
            "  6% 200/3360 [01:12<17:31,  3.01it/s][INFO|trainer.py:2815] 2023-10-31 02:53:19,845 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:53:19,846 >> Configuration saved in bert-base-ml-ner/checkpoint-200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:53:21,304 >> Model weights saved in bert-base-ml-ner/checkpoint-200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:53:30,262 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-3300] due to args.save_total_limit\n",
            "  9% 300/3360 [01:55<16:58,  3.00it/s][INFO|trainer.py:2815] 2023-10-31 02:54:02,854 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:54:02,856 >> Configuration saved in bert-base-ml-ner/checkpoint-300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:54:04,429 >> Model weights saved in bert-base-ml-ner/checkpoint-300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:54:11,370 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-100] due to args.save_total_limit\n",
            " 12% 400/3360 [02:37<14:55,  3.30it/s][INFO|trainer.py:2815] 2023-10-31 02:54:44,333 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:54:44,335 >> Configuration saved in bert-base-ml-ner/checkpoint-400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:54:45,870 >> Model weights saved in bert-base-ml-ner/checkpoint-400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:55:01,749 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 0.0274, 'learning_rate': 2.5535714285714284e-05, 'epoch': 23.81}\n",
            " 15% 500/3360 [03:27<16:02,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:55:34,853 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:55:34,854 >> Configuration saved in bert-base-ml-ner/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:55:36,265 >> Model weights saved in bert-base-ml-ner/checkpoint-500/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:55:39,745 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-300] due to args.save_total_limit\n",
            " 18% 600/3360 [04:05<15:35,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:56:13,009 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-600\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:56:13,010 >> Configuration saved in bert-base-ml-ner/checkpoint-600/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:56:14,381 >> Model weights saved in bert-base-ml-ner/checkpoint-600/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:56:17,899 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-400] due to args.save_total_limit\n",
            " 21% 700/3360 [04:44<14:55,  2.97it/s][INFO|trainer.py:2815] 2023-10-31 02:56:51,433 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-700\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:56:51,434 >> Configuration saved in bert-base-ml-ner/checkpoint-700/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:56:52,871 >> Model weights saved in bert-base-ml-ner/checkpoint-700/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:57:01,331 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-500] due to args.save_total_limit\n",
            " 24% 800/3360 [05:27<13:28,  3.17it/s][INFO|trainer.py:2815] 2023-10-31 02:57:34,652 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-800\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:57:34,653 >> Configuration saved in bert-base-ml-ner/checkpoint-800/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:57:36,102 >> Model weights saved in bert-base-ml-ner/checkpoint-800/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:57:46,473 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-600] due to args.save_total_limit\n",
            " 27% 900/3360 [06:12<13:57,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:58:19,928 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-900\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:58:19,930 >> Configuration saved in bert-base-ml-ner/checkpoint-900/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:58:21,447 >> Model weights saved in bert-base-ml-ner/checkpoint-900/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:58:25,647 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-700] due to args.save_total_limit\n",
            "{'loss': 0.0015, 'learning_rate': 2.107142857142857e-05, 'epoch': 47.62}\n",
            " 30% 1000/3360 [06:51<13:23,  2.94it/s][INFO|trainer.py:2815] 2023-10-31 02:58:59,140 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1000\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:58:59,141 >> Configuration saved in bert-base-ml-ner/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:59:00,580 >> Model weights saved in bert-base-ml-ner/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:59:07,873 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-800] due to args.save_total_limit\n",
            " 33% 1100/3360 [07:34<12:44,  2.95it/s][INFO|trainer.py:2815] 2023-10-31 02:59:41,410 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1100\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 02:59:41,411 >> Configuration saved in bert-base-ml-ner/checkpoint-1100/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 02:59:42,872 >> Model weights saved in bert-base-ml-ner/checkpoint-1100/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 02:59:52,424 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-900] due to args.save_total_limit\n",
            " 36% 1200/3360 [08:18<11:41,  3.08it/s][INFO|trainer.py:2815] 2023-10-31 03:00:25,800 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1200\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 03:00:25,802 >> Configuration saved in bert-base-ml-ner/checkpoint-1200/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 03:00:27,360 >> Model weights saved in bert-base-ml-ner/checkpoint-1200/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 03:00:36,842 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1000] due to args.save_total_limit\n",
            " 39% 1300/3360 [09:03<11:51,  2.90it/s][INFO|trainer.py:2815] 2023-10-31 03:01:10,414 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1300\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 03:01:10,417 >> Configuration saved in bert-base-ml-ner/checkpoint-1300/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 03:01:11,854 >> Model weights saved in bert-base-ml-ner/checkpoint-1300/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 03:01:19,748 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1100] due to args.save_total_limit\n",
            " 42% 1400/3360 [09:45<11:09,  2.93it/s][INFO|trainer.py:2815] 2023-10-31 03:01:53,097 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1400\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 03:01:53,098 >> Configuration saved in bert-base-ml-ner/checkpoint-1400/config.json\n",
            "[INFO|modeling_utils.py:2190] 2023-10-31 03:01:54,439 >> Model weights saved in bert-base-ml-ner/checkpoint-1400/pytorch_model.bin\n",
            "[INFO|trainer.py:2905] 2023-10-31 03:02:03,673 >> Deleting older checkpoint [bert-base-ml-ner/checkpoint-1200] due to args.save_total_limit\n",
            "{'loss': 0.0, 'learning_rate': 1.660714285714286e-05, 'epoch': 71.43}\n",
            " 45% 1500/3360 [10:29<10:27,  2.96it/s][INFO|trainer.py:2815] 2023-10-31 03:02:37,114 >> Saving model checkpoint to bert-base-ml-ner/checkpoint-1500\n",
            "[INFO|configuration_utils.py:461] 2023-10-31 03:02:37,115 >> Configuration saved in bert-base-ml-ner/checkpoint-1500/config.json\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "#model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "# for i in range(100):\n",
        "#   x = 100 + 11999 + 2000\n",
        "#   current_time = time.time()\n",
        "#   elapsed_time = current_time - start_time\n",
        "\n",
        "\n",
        "\n",
        "# (0.0003, 8) learn y batch\n",
        "#epoch = 10\n",
        "corpus = \"ArgPC\"\n",
        "max_test_f1 = 0\n",
        "max_test_accuracy_score = 0\n",
        "max_parametros = \"\"\n",
        "\n",
        "\n",
        "for k in range( len( parameters ) ) :\n",
        "\n",
        "  sumaF1 = 0\n",
        "  sumaAcc = 0\n",
        "\n",
        "\n",
        "  for fold in range(0,10):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    !rm train.txt\n",
        "    !rm test.txt\n",
        "\n",
        "\n",
        "    !python3 preprocess.py /content/drive/MyDrive/Exp_Objetivos2021/folders/fold_{fold}_train.txt $MODEL $MAX_LENGTH > train.txt\n",
        "    !python3 preprocess.py /content/drive/MyDrive/Exp_Objetivos2021/folders/fold_{fold}_test.txt $MODEL $MAX_LENGTH > test.txt\n",
        "\n",
        "\n",
        "    !cat train.txt  test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n",
        "\n",
        "    #epoch = parameters[k][2]\n",
        "    epoch=160\n",
        "\n",
        "\n",
        "\n",
        "    MAX_LENGTH = 128\n",
        "    MODEL = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "    #MODEL = \"chriskhanhtran/spanberta\"\n",
        "    OUTPUT_DIR = \"bert-base-ml-ner\"\n",
        "    BATCH_SIZE = parameters[k][1] #32\n",
        "    NUM_EPOCHS = epoch\n",
        "    SAVE_STEPS = 100\n",
        "    LOGGING_STEPS = 100\n",
        "    SEED = 42\n",
        "    LEARNING_RATE = parameters[k][0]\n",
        "\n",
        "\n",
        "    parametros = f\"epoch_{str(epoch)}_learn_{str(LEARNING_RATE)}_batch_{str(BATCH_SIZE)}_corpus{corpus}_fold{str(fold)}\\n\"\n",
        "\n",
        "\n",
        "    !python3 run_ner.py --data_dir ./ \\\n",
        "    --labels ./labels.txt \\\n",
        "    --model_name_or_path $MODEL \\\n",
        "    --output_dir $OUTPUT_DIR \\\n",
        "    --max_seq_length  $MAX_LENGTH \\\n",
        "    --num_train_epochs $NUM_EPOCHS \\\n",
        "    --per_gpu_train_batch_size $BATCH_SIZE \\\n",
        "    --save_steps $SAVE_STEPS \\\n",
        "    --seed $SEED \\\n",
        "    --do_train \\\n",
        "    --do_predict \\\n",
        "    --overwrite_cache \\\n",
        "    --overwrite_output_dir \\\n",
        "    --save_total_limit 2 \\\n",
        "    --learning_rate $LEARNING_RATE\n",
        "\n",
        "\n",
        "    !cp ./bert-base-ml-ner/config.json  {dir}config_{parametros}.json\n",
        "    !cp ./bert-base-ml-ner/test_predictions.txt   {dir}test_predictions_{parametros}.txt\n",
        "    !cp ./bert-base-ml-ner/test_results.txt   {dir}test_results_{parametros}.txt\n",
        "\n",
        "    archivo_result = \"./bert-base-ml-ner/test_results.txt\"\n",
        "    test_f1_current = 0\n",
        "    test_accuracy_score_current = 0\n",
        "\n",
        "    current_time = time.time()\n",
        "    elapsed_time = current_time - start_time\n",
        "\n",
        "    testsite_array = []\n",
        "    with open(archivo_result) as my_file:\n",
        "        for line in my_file:\n",
        "            testsite_array.append(line.split() )\n",
        "            if line.split()[0] == \"test_f1\":\n",
        "              print(\"test_f1\",line.split()[2],parametros)\n",
        "              test_f1_current = float(line.split()[2])\n",
        "            if line.split()[0] ==  \"test_accuracy_score\":\n",
        "              print(\"test_accuracy_score\",line.split()[2],parametros)\n",
        "              test_accuracy_score_current = float(line.split()[2])\n",
        "\n",
        "    f = open(dir+\"log_exp.txt\", \"a+\")\n",
        "    log_text = f\"{str(test_accuracy_score_current),str(test_f1_current)},{str(epoch)},{str(LEARNING_RATE)},{str(BATCH_SIZE)},{corpus},{str(fold)},time,{str(elapsed_time)}\\n\"\n",
        "    f.write(log_text)\n",
        "    f.close()\n",
        "\n",
        "    # guardo para acumular F1 y acc\n",
        "\n",
        "    sumaAcc += test_accuracy_score_current\n",
        "    sumaF1 += test_f1_current\n",
        "\n",
        "\n",
        "    if max_test_f1 < test_f1_current :\n",
        "      max_test_f1 = test_f1_current\n",
        "      max_parametros = parametros\n",
        "\n",
        "    #**********resultados\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import classification_report\n",
        "    !rm test.txt\n",
        "\n",
        "    !python3 preprocess.py /content/drive/MyDrive/Exp_Objetivos2021/folders/fold_{fold}_test.txt $MODEL $MAX_LENGTH > test.txt\n",
        "    !python3 preprocess.py /content/bert-base-ml-ner/test_predictions.txt $MODEL $MAX_LENGTH > prediction.txt\n",
        "\n",
        "    !cat test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n",
        "\n",
        "\n",
        "    y_true = read_examples_from_file(\"test.txt\")[\"labels\"]\n",
        "    y_pred = read_examples_from_file(\"prediction.txt\")[\"labels\"]\n",
        "    print (\"RESULTADOS FOLDER\", fold)\n",
        "    print(classification_report(np.concatenate(y_true), np.concatenate(y_pred)))\n",
        "\n",
        "    #***********resultados\n",
        "\n",
        "\n",
        "  print(max_test_f1,max_parametros)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dVT91CHe0JZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJS2BRnWe0MH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27RmMlP6e0PQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sDc8HXOe0SS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MYBJ9cue0Vg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIscBVtee0Yo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKYPFMMvHQm9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RereNgfKnEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r7zsqwHKnI6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKC3BbyEKnNE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K1XnVQyKnQP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghA38uDt9HWt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Ai4AUL9Hak"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Tn8wUH9Hdy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv0gGqB221PL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}